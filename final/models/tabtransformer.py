import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (f1_score, roc_auc_score, precision_score, 
                             recall_score, average_precision_score)
import os
import gc
import joblib

DATA_PATH = "./data_pipeline/"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# TabTransformer ëª¨ë¸ êµ¬ì¡° ì •ì˜ (íŒŒì¼ ë‚´ë¶€ì— í¬í•¨)
class TabTransformer(nn.Module):
    def __init__(self, n_cont, embed_dim=32, n_heads=4, n_layers=2):
        super().__init__()
        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ì„ë² ë”©
        self.cont_embed = nn.Linear(n_cont, n_cont * embed_dim)
        
        # Transformer ì¸ì½”ë”
        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=n_heads, batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)
        
        # ìµœì¢… ë¶„ë¥˜ í—¤ë“œ
        self.mlp = nn.Sequential(
            nn.Linear(n_cont * embed_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        batch_size = x.size(0)
        # (batch, n_cont) -> (batch, n_cont, embed_dim)
        x = self.cont_embed(x).view(batch_size, -1, 32)
        x = self.transformer(x)
        x = x.view(batch_size, -1) # Flatten
        return self.mlp(x)

def evaluate_tt():
    data_files = [("SMOTE", "train_smote.csv"), ("cGAN", "train_cgan.csv"), ("K-cGAN", "train_kcgan.csv")]
    final_results = []

    for name, file_name in data_files:
        path = os.path.join(DATA_PATH, file_name)
        if not os.path.exists(path): continue

        print(f"ğŸš€ [{name}] TabTransformer í•™ìŠµ ë° í‰ê°€ ì¤‘...")
        df = pd.read_csv(path)
        X = df.drop('Class', axis=1).values.astype(np.float32)
        y = df['Class'].values.astype(int)

        # 8:2 ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

        # ì „ì²˜ë¦¬ (Standard Scaling)
        scaler = StandardScaler()
        X_train_s = scaler.fit_transform(X_train)
        X_test_s = scaler.transform(X_test)

        # ë°ì´í„° ë¡œë” ì¤€ë¹„
        loader = DataLoader(TensorDataset(torch.FloatTensor(X_train_s).to(device), 
                                          torch.FloatTensor(y_train).view(-1, 1).to(device)), 
                            batch_size=1024, shuffle=True)

        # ëª¨ë¸ ìƒì„±
        model = TabTransformer(n_cont=X_train.shape[1]).to(device)
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        criterion = nn.BCELoss()

        # í•™ìŠµ (30 Epoch)
        model.train()
        for epoch in range(30):
            for bx, by in loader:
                optimizer.zero_grad()
                output = model(bx)
                loss = criterion(output, by)
                loss.backward(); optimizer.step()

        # í‰ê°€
        model.eval()
        with torch.no_grad():
            X_test_tensor = torch.FloatTensor(X_test_s).to(device)
            probs = model(X_test_tensor).cpu().numpy()
            preds = (probs > 0.5).astype(int)

        final_results.append({
            "Method": name,
            "Precision": precision_score(y_test, preds),
            "Recall": recall_score(y_test, preds),
            "F1-Score": f1_score(y_test, preds),
            "ROC-AUC": roc_auc_score(y_test, probs),
            "AUPRC": average_precision_score(y_test, probs)
        })
        del df, X_train, X_test; gc.collect()

    report_df = pd.DataFrame(final_results)
    print("\n" + "="*70)
    print("ğŸ“Š TabTransformer ê¸°ë°˜ ì¦ê°• ê¸°ë²•ë³„ ì„±ëŠ¥ ë¹„êµ (8:2 Split)")
    print("="*70)
    print(report_df.to_string(index=False))

if __name__ == "__main__":
    evaluate_tt()
import pandas as pd
import numpy as np
import joblib
import gc
from sklearn.metrics import precision_recall_curve, f1_score, roc_auc_score, recall_score, precision_score

# --- [1. ì¥ì¹˜ ì„¤ì • ë° ê³µí†µ ë°ì´í„° ë¡œë“œ] ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
data_variants = ['org', 'smote', 'cgan', 'kcgan']

print(f"ğŸ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¡œë“œ ì¤‘... (Device: {device})")
X_test_scaled = joblib.load('X_test_scaled.pkl')
y_test = joblib.load('y_test.pkl')
X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)

# --- [2. TabTransformer ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ì˜] ---
# Transformer Encoderë¥¼ ì‚¬ìš©í•˜ì—¬ í”¼ì²˜ ê°„ì˜ ìƒí˜¸ì‘ìš©ì„ í•™ìŠµí•©ë‹ˆë‹¤.
class TabTransformer(nn.Module):
    def __init__(self, input_dim, embed_dim, num_heads, num_layers, dropout=0.1):
        super(TabTransformer, self).__init__()
        self.embed_dim = embed_dim
        # ìˆ˜ì¹˜í˜• í”¼ì²˜ë¥¼ ì„ë² ë”© ê³µê°„ìœ¼ë¡œ íˆ¬ì‚¬
        self.input_projection = nn.Linear(input_dim, input_dim * embed_dim)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim, nhead=num_heads, dim_feedforward=embed_dim * 4,
            dropout=dropout, batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        self.mlp = nn.Sequential(
            nn.Linear(input_dim * embed_dim, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1) # ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì¶œë ¥ì¸µ
        )

    def forward(self, x):
        # [Batch, Input_Dim] -> [Batch, Input_Dim, Embed_Dim]
        x = self.input_projection(x).view(x.size(0), -1, self.embed_dim)
        x = self.transformer_encoder(x)
        x = x.reshape(x.size(0), -1)
        return self.mlp(x)

# --- [3. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ë£¨í”„] ---
tab_results = []

for variant in data_variants:
    print(f"\n" + "="*50)
    print(f"ğŸš€ TabTransformer í•™ìŠµ ì‹œì‘: [{variant.upper()}] ë°ì´í„°ì…‹")
    print("="*50)
    
    # 1) ë°ì´í„°ì…‹ ë¡œë“œ
    X_tr = joblib.load(f'X_train_{variant}.pkl')
    y_tr = joblib.load(f'y_train_{variant}.pkl')
    
    input_dim = X_tr.shape[1]
    train_loader = DataLoader(
        TensorDataset(torch.FloatTensor(X_tr), torch.FloatTensor(y_tr).view(-1, 1)), 
        batch_size=1024, shuffle=True
    )
    
    # 2) ëª¨ë¸ ë° ìµœì í™” ì„¤ì •
    # pos_weight=4.0: ì‚¬ê¸° ë°ì´í„°(1)ì— 4ë°°ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë‘ì–´ ë¶ˆê· í˜• í•´ì†Œ
    model = TabTransformer(input_dim, embed_dim=32, num_heads=8, num_layers=3).to(device)
    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([4.0]).to(device))
    optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)

    # 3) í•™ìŠµ (Training)
    model.train()
    for epoch in range(30): # 30 ì—í¬í¬ í•™ìŠµ
        total_loss = 0
        for bx, by in train_loader:
            bx, by = bx.to(device), by.to(device)
            optimizer.zero_grad()
            outputs = model(bx)
            loss = criterion(outputs, by)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        scheduler.step()
        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/30] - Loss: {total_loss/len(train_loader):.4f}")

    # 4) í‰ê°€ (Evaluation)
    model.eval()
    with torch.no_grad():
        # ì˜ˆì¸¡ í™•ë¥  ê³„ì‚° (Sigmoid ì ìš©)
        logits = model(X_test_tensor)
        tab_probs = torch.sigmoid(logits).cpu().numpy().flatten()
    
    # Precision-Recall Curveë¥¼ í†µí•´ ìµœì ì˜ F1-Scoreì™€ ì„ê³„ê°’ ì°¾ê¸°
    precisions, recalls, thresholds = precision_recall_curve(y_test, tab_probs)
    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)
    best_idx = np.argmax(f1_scores)
    
    tab_results.append({
        "Dataset": variant.upper(),
        "F1-Score": f1_scores[best_idx],
        "Recall": recalls[best_idx],
        "Precision": precisions[best_idx],
        "ROC-AUC": roc_auc_score(y_test, tab_probs),
        "Best_Threshold": thresholds[best_idx] if best_idx < len(thresholds) else 0.5
    })

    # 5) ë©”ëª¨ë¦¬ ì •ë¦¬
    del X_tr, y_tr, model, train_loader
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

# --- [4. ìµœì¢… ê²°ê³¼ ì¶œë ¥] ---
print("\n" + "âœ¨" * 25)
print("ğŸ† TabTransformer ìµœì¢… ì‹¤í—˜ ê²°ê³¼ ë¦¬í¬íŠ¸")
print("âœ¨" * 25)
tab_df = pd.DataFrame(tab_results)
pd.options.display.float_format = '{:.4f}'.format
print(tab_df.sort_values(by="F1-Score", ascending=False).to_string(index=False))
