{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95caa0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ [SMOTE] RF ÌïôÏäµ Î∞è ÌèâÍ∞Ä Ï§ë...\n",
      "üöÄ [cGAN] RF ÌïôÏäµ Î∞è ÌèâÍ∞Ä Ï§ë...\n",
      "üöÄ [K-cGAN] RF ÌïôÏäµ Î∞è ÌèâÍ∞Ä Ï§ë...\n",
      "\n",
      "======================================================================\n",
      "üìä Random Forest Í∏∞Î∞ò Ï¶ùÍ∞ï Í∏∞Î≤ïÎ≥Ñ ÏÑ±Îä• ÎπÑÍµê (8:2 Split)\n",
      "======================================================================\n",
      "Method  Precision   Recall  F1-Score  ROC-AUC   AUPRC\n",
      " SMOTE   0.999736 1.000000  0.999868 1.000000 1.00000\n",
      "  cGAN   0.999930 0.999648  0.999789 0.999918 0.99992\n",
      "K-cGAN   0.999912 0.999648  0.999780 0.999918 0.99992\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (f1_score, roc_auc_score, precision_score, \n",
    "                             recall_score, average_precision_score)\n",
    "\n",
    "DATA_PATH = \"./data_pipeline/\"\n",
    "\n",
    "def evaluate_rf():\n",
    "    data_files = [(\"SMOTE\", \"train_smote.csv\"), (\"cGAN\", \"train_cgan.csv\"), (\"K-cGAN\", \"train_kcgan.csv\")]\n",
    "    final_results = []\n",
    "\n",
    "    for name, file_name in data_files:\n",
    "        path = os.path.join(DATA_PATH, file_name)\n",
    "        if not os.path.exists(path): continue\n",
    "\n",
    "        print(f\"üöÄ [{name}] RF ÌïôÏäµ Î∞è ÌèâÍ∞Ä Ï§ë...\")\n",
    "        df = pd.read_csv(path)\n",
    "        X = df.drop('Class', axis=1).values.astype(np.float32)\n",
    "        y = df['Class'].values.astype(int)\n",
    "\n",
    "        # 8:2 Î∂ÑÌï†\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "        # Î™®Îç∏ ÌïôÏäµ (Random Forest)\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # ÌèâÍ∞Ä\n",
    "        preds = model.predict(X_test)\n",
    "        probs = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        final_results.append({\n",
    "            \"Method\": name,\n",
    "            \"Precision\": precision_score(y_test, preds),\n",
    "            \"Recall\": recall_score(y_test, preds),\n",
    "            \"F1-Score\": f1_score(y_test, preds),\n",
    "            \"ROC-AUC\": roc_auc_score(y_test, probs),\n",
    "            \"AUPRC\": average_precision_score(y_test, probs)\n",
    "        })\n",
    "        del df, X_train, X_test; gc.collect()\n",
    "\n",
    "    report_df = pd.DataFrame(final_results)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä Random Forest Í∏∞Î∞ò Ï¶ùÍ∞ï Í∏∞Î≤ïÎ≥Ñ ÏÑ±Îä• ÎπÑÍµê (8:2 Split)\")\n",
    "    print(\"=\"*70)\n",
    "    print(report_df.to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_rf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd41611f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ [SMOTE] TabTransformer ÌïôÏäµ Î∞è ÌèâÍ∞Ä Ï§ë...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 110\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28mprint\u001b[39m(report_df\u001b[38;5;241m.\u001b[39mto_string(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 110\u001b[0m     evaluate_tt()\n",
      "Cell \u001b[1;32mIn[7], line 82\u001b[0m, in \u001b[0;36mevaluate_tt\u001b[1;34m()\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bx, by \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m     81\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 82\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(bx)\n\u001b[0;32m     83\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, by)\n\u001b[0;32m     84\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(); optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\sunhe\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sunhe\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[7], line 42\u001b[0m, in \u001b[0;36mTabTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# (batch, n_cont) -> (batch, n_cont, embed_dim)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcont_embed(x)\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m---> 42\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(x)\n\u001b[0;32m     43\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Flatten\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(x)\n",
      "File \u001b[1;32mc:\\Users\\sunhe\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sunhe\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sunhe\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:524\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    521\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 524\u001b[0m     output \u001b[38;5;241m=\u001b[39m mod(\n\u001b[0;32m    525\u001b[0m         output,\n\u001b[0;32m    526\u001b[0m         src_mask\u001b[38;5;241m=\u001b[39mmask,\n\u001b[0;32m    527\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m    528\u001b[0m         src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask_for_layers,\n\u001b[0;32m    529\u001b[0m     )\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    532\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.0\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32mc:\\Users\\sunhe\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sunhe\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sunhe\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:937\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    933\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[0;32m    934\u001b[0m         x\n\u001b[0;32m    935\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m    936\u001b[0m     )\n\u001b[1;32m--> 937\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\sunhe\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:962\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._ff_block\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 962\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(x))))\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[1;32mc:\\Users\\sunhe\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\sunhe\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sunhe\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m     70\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[1;32mc:\\Users\\sunhe\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:1418\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m-> 1418\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28minput\u001b[39m, p, training)\n\u001b[0;32m   1419\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (f1_score, roc_auc_score, precision_score, \n",
    "                             recall_score, average_precision_score)\n",
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "\n",
    "DATA_PATH = \"./data_pipeline/\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# TabTransformer Î™®Îç∏ Íµ¨Ï°∞ Ï†ïÏùò (ÌååÏùº ÎÇ¥Î∂ÄÏóê Ìè¨Ìï®)\n",
    "class TabTransformer(nn.Module):\n",
    "    def __init__(self, n_cont, embed_dim=32, n_heads=4, n_layers=2):\n",
    "        super().__init__()\n",
    "        # ÏàòÏπòÌòï Î≥ÄÏàò ÏûÑÎ≤†Îî©\n",
    "        self.cont_embed = nn.Linear(n_cont, n_cont * embed_dim)\n",
    "        \n",
    "        # Transformer Ïù∏ÏΩîÎçî\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=n_heads, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        # ÏµúÏ¢Ö Î∂ÑÎ•ò Ìó§Îìú\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_cont * embed_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        # (batch, n_cont) -> (batch, n_cont, embed_dim)\n",
    "        x = self.cont_embed(x).view(batch_size, -1, 32)\n",
    "        x = self.transformer(x)\n",
    "        x = x.view(batch_size, -1) # Flatten\n",
    "        return self.mlp(x)\n",
    "\n",
    "def evaluate_tt():\n",
    "    data_files = [(\"SMOTE\", \"train_smote.csv\"), (\"cGAN\", \"train_cgan.csv\"), (\"K-cGAN\", \"train_kcgan.csv\")]\n",
    "    final_results = []\n",
    "\n",
    "    for name, file_name in data_files:\n",
    "        path = os.path.join(DATA_PATH, file_name)\n",
    "        if not os.path.exists(path): continue\n",
    "\n",
    "        print(f\"üöÄ [{name}] TabTransformer ÌïôÏäµ Î∞è ÌèâÍ∞Ä Ï§ë...\")\n",
    "        df = pd.read_csv(path)\n",
    "        X = df.drop('Class', axis=1).values.astype(np.float32)\n",
    "        y = df['Class'].values.astype(int)\n",
    "\n",
    "        # 8:2 Î∂ÑÌï†\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "        # Ï†ÑÏ≤òÎ¶¨ (Standard Scaling)\n",
    "        scaler = StandardScaler()\n",
    "        X_train_s = scaler.fit_transform(X_train)\n",
    "        X_test_s = scaler.transform(X_test)\n",
    "\n",
    "        # Îç∞Ïù¥ÌÑ∞ Î°úÎçî Ï§ÄÎπÑ\n",
    "        loader = DataLoader(TensorDataset(torch.FloatTensor(X_train_s).to(device), \n",
    "                                          torch.FloatTensor(y_train).view(-1, 1).to(device)), \n",
    "                            batch_size=1024, shuffle=True)\n",
    "\n",
    "        # Î™®Îç∏ ÏÉùÏÑ±\n",
    "        model = TabTransformer(n_cont=X_train.shape[1]).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        # ÌïôÏäµ (30 Epoch)\n",
    "        model.train()\n",
    "        for epoch in range(30):\n",
    "            for bx, by in loader:\n",
    "                optimizer.zero_grad()\n",
    "                output = model(bx)\n",
    "                loss = criterion(output, by)\n",
    "                loss.backward(); optimizer.step()\n",
    "\n",
    "        # ÌèâÍ∞Ä\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_test_tensor = torch.FloatTensor(X_test_s).to(device)\n",
    "            probs = model(X_test_tensor).cpu().numpy()\n",
    "            preds = (probs > 0.5).astype(int)\n",
    "\n",
    "        final_results.append({\n",
    "            \"Method\": name,\n",
    "            \"Precision\": precision_score(y_test, preds),\n",
    "            \"Recall\": recall_score(y_test, preds),\n",
    "            \"F1-Score\": f1_score(y_test, preds),\n",
    "            \"ROC-AUC\": roc_auc_score(y_test, probs),\n",
    "            \"AUPRC\": average_precision_score(y_test, probs)\n",
    "        })\n",
    "        del df, X_train, X_test; gc.collect()\n",
    "\n",
    "    report_df = pd.DataFrame(final_results)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä TabTransformer Í∏∞Î∞ò Ï¶ùÍ∞ï Í∏∞Î≤ïÎ≥Ñ ÏÑ±Îä• ÎπÑÍµê (8:2 Split)\")\n",
    "    print(\"=\"*70)\n",
    "    print(report_df.to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_tt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92de0798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
