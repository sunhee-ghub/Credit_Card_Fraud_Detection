{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7513438f-c8cc-4724-b1cd-a72e2ea2b827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì…€ 1: ê³µí†µ ì „ì²˜ë¦¬ ì™„ë£Œ (X_train_scaled, X_test_scaled ìƒì„±ë¨)\n"
     ]
    }
   ],
   "source": [
    "# 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import pandas as pd # ë°ì´í„° í•¸ë“¤ë§ìš©\n",
    "import numpy as np # ìˆ˜ì¹˜ ì—°ì‚°ìš©\n",
    "from sklearn.model_selection import train_test_split # ë°ì´í„°ì…‹ ë¶„í• ìš©\n",
    "from sklearn.preprocessing import StandardScaler # ìˆ˜ì¹˜í˜• ë°ì´í„° ìŠ¤ì¼€ì¼ë§ìš©\n",
    "\n",
    "# 2. ì‹ ìš©ì¹´ë“œ ê±°ë˜ ë°ì´í„° ë¡œë“œ (íŒŒì¼ ê²½ë¡œ í™•ì¸ í•„ìš”)\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "\n",
    "# 3. ì‹œê°„(Time) ë°ì´í„°ë¥¼ 24ì‹œê°„ ì£¼ê¸°ì˜ ì‹œê°„ëŒ€(Hour) ì •ë³´ë¡œ ë³€í™˜ (íŒ¨í„´ ì¶”ì¶œ)\n",
    "df['Hour'] = (df['Time'] // 3600) % 24\n",
    "\n",
    "# 4. ê²°ì œ ê¸ˆì•¡(Amount)ì˜ í° ìˆ˜ì¹˜ í¸ì°¨ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë¡œê·¸ ë³€í™˜(Log Transformation) ì ìš©\n",
    "df['Log_Amount'] = np.log1p(df['Amount'])\n",
    "\n",
    "# 5. ëª¨ë¸ í•™ìŠµì— ë¶ˆí•„ìš”í•œ ì›ë³¸ ì»¬ëŸ¼(Time, Amount) ë° ì •ë‹µ(Class) ì œì™¸í•˜ì—¬ í”¼ì²˜ ì„¸íŠ¸ êµ¬ì¶•\n",
    "X = df.drop(['Class', 'Time', 'Amount'], axis=1)\n",
    "\n",
    "# 6. ì •ë‹µ ë ˆì´ë¸”(0: ì •ìƒ, 1: ì‚¬ê¸°)ë§Œ ë³„ë„ë¡œ ë¶„ë¦¬\n",
    "y = df['Class']\n",
    "\n",
    "# 7. í˜„ì‹¤ì ì¸ ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•´ í…ŒìŠ¤íŠ¸ ë°ì´í„°(20%)ë¥¼ ë¨¼ì € ë¶„ë¦¬ (ì‚¬ê¸° ë¹„ìœ¨ ìœ ì§€)\n",
    "# ì´í›„ ì¦ê°• ì‘ì—…ì€ ì˜¤ì§ í•™ìŠµ ë°ì´í„°(X_train, y_train)ì—ë§Œ ìˆ˜í–‰í•¨\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# 8. DL ëª¨ë¸ê³¼ íŠ¸ë¦¬ ëª¨ë¸ ëª¨ë‘ì— ì í•©í•˜ë„ë¡ ë°ì´í„° í‘œì¤€í™”(Scaling) ìˆ˜í–‰\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train) # í•™ìŠµ ë°ì´í„° ê¸°ì¤€ í•™ìŠµ ë° ë³€í™˜\n",
    "X_test_scaled = scaler.transform(X_test) # í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” í•™ìŠµ ê¸°ì¤€ì— ë§ì¶° ë³€í™˜\n",
    "\n",
    "print(\"âœ… ì…€ 1: ê³µí†µ ì „ì²˜ë¦¬ ì™„ë£Œ (X_train_scaled, X_test_scaled ìƒì„±ë¨)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a12ba9c9-9edb-4d3b-b893-65ad441b2f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì…€ 2: 4ì¢… ë°ì´í„° ì¦ê°• ì™„ë£Œ!\n",
      "- Original: ë°ì´í„° 227845ê±´ (ì‚¬ê¸° ë¹„ì¤‘: 0.2%)\n",
      "- SMOTE: ë°ì´í„° 272941ê±´ (ì‚¬ê¸° ë¹„ì¤‘: 16.7%)\n",
      "- cGAN: ë°ì´í„° 272941ê±´ (ì‚¬ê¸° ë¹„ì¤‘: 16.7%)\n",
      "- K-cGAN: ë°ì´í„° 272935ê±´ (ì‚¬ê¸° ë¹„ì¤‘: 16.7%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunhe\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1. ì¶”ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (ì¦ê°• ë„êµ¬)\n",
    "from imblearn.over_sampling import SMOTE # SMOTE ì¦ê°•ìš©\n",
    "from sklearn.cluster import KMeans # K-cGAN êµ°ì§‘í™”ìš©\n",
    "\n",
    "# 2. ì¦ê°• ëª©í‘œ ë¹„ìœ¨ ì„¤ì • (ì •ìƒ ë°ì´í„°ì˜ 20% ìˆ˜ì¤€ìœ¼ë¡œ ì‚¬ê¸° ë°ì´í„° í™•ë³´)\n",
    "target_ratio = 0.2\n",
    "\n",
    "# --- [ë°©ë²• A] Original: ì¦ê°•í•˜ì§€ ì•Šì€ ë¶ˆê· í˜• ì›ë³¸ í•™ìŠµ ë°ì´í„° ---\n",
    "# ë³€ìˆ˜ëª… í†µì¼ì„ ìœ„í•´ ì›ë³¸ ê·¸ëŒ€ë¡œ ë³µì‚¬\n",
    "X_train_org, y_train_org = X_train_scaled, y_train.values\n",
    "\n",
    "# --- [ë°©ë²• B] SMOTE: ì„ í˜• ë³´ê°„ì„ ì´ìš©í•œ ì •ì„ì ì¸ ì¦ê°• ---\n",
    "smote = SMOTE(sampling_strategy=target_ratio, random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# --- [ë°©ë²• C] cGAN: ì „ì²´ ì‚¬ê¸° ë°ì´í„°ì˜ í†µê³„ê°’(í‰ê· /í¸ì°¨)ì„ ì´ìš©í•œ ë‹¨ìˆœ ìƒì„± ---\n",
    "# ì‹¤ì œ ì‚¬ê¸° ë°ì´í„°ì˜ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°\n",
    "fraud_indices = np.where(y_train == 1)[0]\n",
    "fraud_mean = X_train_scaled[fraud_indices].mean(axis=0)\n",
    "fraud_std = X_train_scaled[fraud_indices].std(axis=0)\n",
    "# ìƒì„±í•´ì•¼ í•  ê°œìˆ˜ ì‚°ì¶œ (ì •ìƒì˜ 20% - í˜„ì¬ ì‚¬ê¸° ìˆ˜)\n",
    "needed_cgan = int(len(X_train_scaled[y_train == 0]) * target_ratio) - len(fraud_indices)\n",
    "# ì •ê·œë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì§œ ì‚¬ê¸° ë°ì´í„° ìƒì„±\n",
    "fake_cgan = np.random.normal(fraud_mean, fraud_std * 0.25, size=(needed_cgan, X_train_scaled.shape[1]))\n",
    "# ì›ë³¸ê³¼ ê²°í•©\n",
    "X_train_cgan = np.vstack([X_train_scaled, fake_cgan])\n",
    "y_train_cgan = np.append(y_train.values, np.ones(needed_cgan))\n",
    "\n",
    "# --- [ë°©ë²• D] K-cGAN: êµ°ì§‘í™”(K-Means) í›„ ê° êµ°ì§‘ë³„ ê°œë³„ ìƒì„± (ê°€ì¥ ì •êµí•¨) ---\n",
    "X_fraud_raw = X_train_scaled[fraud_indices]\n",
    "# ì‚¬ê¸° íŒ¨í„´ì„ 10ê°œë¡œ ìª¼ê°œì–´ í•™ìŠµ (ì„¸ë¶€ íŠ¹ì§• ë³´ì¡´)\n",
    "kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_fraud_raw)\n",
    "needed_kcgan = int(len(X_train_scaled[y_train == 0]) * target_ratio) - len(fraud_indices)\n",
    "gen_per_cluster = needed_kcgan // 10\n",
    "gen_samples_kcgan = []\n",
    "for i in range(10): # ê° ì‚¬ê¸° ê·¸ë£¹ë³„ë¡œ ë°˜ë³µ ìƒì„±\n",
    "    cluster_subset = X_fraud_raw[clusters == i]\n",
    "    fake_subset = np.random.normal(cluster_subset.mean(axis=0), cluster_subset.std(axis=0) * 0.25, size=(gen_per_cluster, X_train_scaled.shape[1]))\n",
    "    gen_samples_kcgan.append(fake_subset)\n",
    "# ìµœì¢… K-cGAN ë°ì´í„°ì…‹ ê²°í•©\n",
    "X_train_kcgan = np.vstack([X_train_scaled, np.vstack(gen_samples_kcgan)])\n",
    "y_train_kcgan = np.append(y_train.values, np.ones(len(np.vstack(gen_samples_kcgan))))\n",
    "\n",
    "# 3. ëª¨ë“  ë°ì´í„°ì…‹ì„ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥í•˜ì—¬ ë¹„êµ ì‹¤í—˜ ì¤€ë¹„ ì™„ë£Œ\n",
    "experimental_sets = {\n",
    "    \"Original\": (X_train_org, y_train_org),\n",
    "    \"SMOTE\": (X_train_smote, y_train_smote),\n",
    "    \"cGAN\": (X_train_cgan, y_train_cgan),\n",
    "    \"K-cGAN\": (X_train_kcgan, y_train_kcgan)\n",
    "}\n",
    "\n",
    "print(f\"âœ… ì…€ 2: 4ì¢… ë°ì´í„° ì¦ê°• ì™„ë£Œ!\")\n",
    "for name, (X_tr, y_tr) in experimental_sets.items():\n",
    "    print(f\"- {name}: ë°ì´í„° {len(X_tr)}ê±´ (ì‚¬ê¸° ë¹„ì¤‘: {np.mean(y_tr)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15308e5f-e07c-4184-8b0f-e5da385a31f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ TabTransformer(BERT ê¸°ë°˜) í•™ìŠµ ì‹œì‘ (Device: cpu)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 85\u001b[0m\n\u001b[0;32m     82\u001b[0m batch_x, batch_y \u001b[38;5;241m=\u001b[39m batch_x\u001b[38;5;241m.\u001b[39mto(device), batch_y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     84\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# ê¸°ìš¸ê¸° ì´ˆê¸°í™”\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(batch_x) \u001b[38;5;66;03m# ì˜ˆì¸¡ê°’ ê³„ì‚°\u001b[39;00m\n\u001b[0;32m     86\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y) \u001b[38;5;66;03m# ì˜¤ì°¨ ê³„ì‚°\u001b[39;00m\n\u001b[0;32m     87\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# ì—­ì „íŒŒ (ì˜¤ì°¨ ì „íŒŒ)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[4], line 41\u001b[0m, in \u001b[0;36mTabTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_projection(x)\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, embed_dim)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# 2. Transformerë¥¼ í†µê³¼í•˜ë©° í”¼ì²˜ ê°„ ìƒí˜¸ì‘ìš© í•™ìŠµ (Attention ìˆ˜í–‰)\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder(x)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# 3. ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì¼ë ¬ë¡œ í´ì„œ(Flatten) ë¶„ë¥˜ê¸°ë¡œ ì „ë‹¬\u001b[39;00m\n\u001b[0;32m     43\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:524\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    521\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 524\u001b[0m     output \u001b[38;5;241m=\u001b[39m mod(\n\u001b[0;32m    525\u001b[0m         output,\n\u001b[0;32m    526\u001b[0m         src_mask\u001b[38;5;241m=\u001b[39mmask,\n\u001b[0;32m    527\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m    528\u001b[0m         src_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_key_padding_mask_for_layers,\n\u001b[0;32m    529\u001b[0m     )\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[0;32m    532\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.0\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:937\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    933\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(\n\u001b[0;32m    934\u001b[0m         x\n\u001b[0;32m    935\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m    936\u001b[0m     )\n\u001b[1;32m--> 937\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:962\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._ff_block\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 962\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(x))))\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:1697\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1695\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1696\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1697\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1698\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import torch # íŒŒì´í† ì¹˜ ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import torch.nn as nn # ì‹ ê²½ë§ ë ˆì´ì–´ ëª¨ë“ˆ\n",
    "import torch.optim as optim # ìµœì í™” ì•Œê³ ë¦¬ì¦˜\n",
    "from torch.utils.data import DataLoader, TensorDataset # ë°ì´í„° ë¡œë”© ë„êµ¬\n",
    "import numpy as np # ìˆ˜ì¹˜ ì—°ì‚°\n",
    "\n",
    "# 2. TabTransformer ì•„í‚¤í…ì²˜ ì •ì˜ (ìˆ«ìí˜• ë°ì´í„° ê°„ì˜ ê´€ê³„ë¥¼ Attentionìœ¼ë¡œ íŒŒì•…)\n",
    "class TabTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, num_layers, dropout=0.1):\n",
    "        super(TabTransformer, self).__init__()\n",
    "        \n",
    "        # [Projection Step] ì…ë ¥ í”¼ì²˜ë“¤ì„ Transformerê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ì„ë² ë”© ì°¨ì›ìœ¼ë¡œ í™•ì¥\n",
    "        self.input_projection = nn.Linear(input_dim, input_dim * embed_dim)\n",
    "        \n",
    "        # [Transformer ë ˆì´ì–´ ì„¤ì •] BERTì˜ í•µì‹¬ì¸ 'Self-Attention' êµ¬ì¡°\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, # ì„ë² ë”© ì°¨ì›\n",
    "            nhead=num_heads, # ê´€ê³„ë¥¼ íŒŒì•…í•  í—¤ë“œì˜ ê°œìˆ˜ (ë©€í‹°í—¤ë“œ)\n",
    "            dim_feedforward=embed_dim * 4, # ë‚´ë¶€ ì‹ ê²½ë§ í¬ê¸°\n",
    "            dropout=dropout, # ê³¼ì í•© ë°©ì§€ ë¹„ìœ¨\n",
    "            batch_first=True # (ë°°ì¹˜, ì‹œí€€ìŠ¤, íŠ¹ì§•) ìˆœì„œ ìœ ì§€\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # [MLP ë¶„ë¥˜ê¸°] ì¶”ì¶œëœ íŠ¹ì§•ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì‚¬ê¸° ì—¬ë¶€ë¥¼ íŒë³„í•˜ëŠ” ì‹ ê²½ë§\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim * embed_dim, 128), # ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ\n",
    "            nn.ReLU(), # í™œì„±í™” í•¨ìˆ˜\n",
    "            nn.Dropout(dropout), # ê³¼ì í•© ë°©ì§€\n",
    "            nn.Linear(128, 64), # ë‘ ë²ˆì§¸ ì€ë‹‰ì¸µ\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1), # ì¶œë ¥ì¸µ (ì‚¬ê¸°ì¼ í™•ë¥  1ê°œ ê°’)\n",
    "            nn.Sigmoid() # 0~1 ì‚¬ì´ í™•ë¥ ê°’ìœ¼ë¡œ ë³€í™˜\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. ì…ë ¥ ë°ì´í„°ë¥¼ (ë°°ì¹˜, í”¼ì²˜ìˆ˜, ì„ë² ë”©ì°¨ì›) í˜•íƒœë¡œ ì¬êµ¬ì„±\n",
    "        x = self.input_projection(x).view(x.size(0), -1, embed_dim)\n",
    "        # 2. Transformerë¥¼ í†µê³¼í•˜ë©° í”¼ì²˜ ê°„ ìƒí˜¸ì‘ìš© í•™ìŠµ (Attention ìˆ˜í–‰)\n",
    "        x = self.transformer_encoder(x)\n",
    "        # 3. ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì¼ë ¬ë¡œ í´ì„œ(Flatten) ë¶„ë¥˜ê¸°ë¡œ ì „ë‹¬\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        # 4. ìµœì¢… ì‚¬ê¸° í™•ë¥ ê°’ ë°˜í™˜\n",
    "        return self.mlp(x)\n",
    "\n",
    "# 3. ë°ì´í„° ë¡œë” ì¤€ë¹„ (K-cGAN ì¦ê°• ë°ì´í„° í™œìš©)\n",
    "# ì•ì„  ì…€ì—ì„œ ìƒì„±í•œ K-cGAN ë°ì´í„°ì…‹ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "X_tr_kcgan, y_tr_kcgan = experimental_sets[\"K-cGAN\"]\n",
    "\n",
    "# PyTorch ì—°ì‚°ì„ ìœ„í•´ numpy ë°°ì—´ì„ í…ì„œ(Tensor) í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "X_train_tensor = torch.FloatTensor(X_tr_kcgan)\n",
    "y_train_tensor = torch.FloatTensor(y_tr_kcgan).view(-1, 1)\n",
    "# ì£¼ì˜: ì…€ 1ì—ì„œ ë§Œë“  'X_test_scaled' ë³€ìˆ˜ëª…ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).view(-1, 1)\n",
    "\n",
    "# ë°°ì¹˜(Batch) ë‹¨ìœ„ë¡œ í•™ìŠµí•˜ê¸° ìœ„í•œ ë°ì´í„°ì…‹ êµ¬ì¶•\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "# 4. ëª¨ë¸ ìƒì„± ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "input_dim = X_train_scaled.shape[1] # ì…ë ¥ í”¼ì²˜ ê°œìˆ˜ (30ê°œ)\n",
    "embed_dim = 16 # ê° í”¼ì²˜ë‹¹ ì„ë² ë”© ì°¨ì›\n",
    "num_heads = 4 # Attention í—¤ë“œ ìˆ˜\n",
    "num_layers = 2 # Transformer ë¸”ë¡ ì¸µ ìˆ˜\n",
    "\n",
    "# GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ GPUë¡œ, ì•„ë‹ˆë©´ CPUë¡œ ì—°ì‚° ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TabTransformer(input_dim, embed_dim, num_heads, num_layers).to(device)\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜(ì´ì§„ í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼) ë° ìµœì í™” ë„êµ¬(Adam) ì„¤ì •\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 5. ëª¨ë¸ í•™ìŠµ ë£¨í”„ ì‹œì‘\n",
    "print(f\"ğŸš€ TabTransformer(BERT ê¸°ë°˜) í•™ìŠµ ì‹œì‘ (Device: {device})...\")\n",
    "model.train() # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜\n",
    "for epoch in range(15): # 15íšŒ ë°˜ë³µ í•™ìŠµ\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad() # ê¸°ìš¸ê¸° ì´ˆê¸°í™”\n",
    "        outputs = model(batch_x) # ì˜ˆì¸¡ê°’ ê³„ì‚°\n",
    "        loss = criterion(outputs, batch_y) # ì˜¤ì°¨ ê³„ì‚°\n",
    "        loss.backward() # ì—­ì „íŒŒ (ì˜¤ì°¨ ì „íŒŒ)\n",
    "        optimizer.step() # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/15], í‰ê·  Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print(\"âœ… í•™ìŠµ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23d7486-c389-4a30-b29d-23317155c7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜ (Dropout ë“± ë¹„í™œì„±í™”)\n",
    "model.eval()\n",
    "with torch.no_grad(): # ê¸°ìš¸ê¸° ê³„ì‚° ì œì™¸ (ì—°ì‚° ì†ë„ í–¥ìƒ)\n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ í™•ë¥ ê°’ ì˜ˆì¸¡\n",
    "    test_outputs = model(X_test_tensor.to(device)).cpu().numpy()\n",
    "\n",
    "# 2. ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° (F1-Scoreë¥¼ ìµœëŒ€ë¡œ í•˜ëŠ” ìµœì  ì„ê³„ê°’ íƒìƒ‰)\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, roc_auc_score\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, test_outputs)\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)\n",
    "best_idx = np.argmax(f1_scores) # ê°€ì¥ ë†’ì€ F1ì˜ ì¸ë±ìŠ¤\n",
    "\n",
    "# 3. ìµœì¢… ê²°ê³¼ ì¶œë ¥\n",
    "print(\"\\n[ğŸ“Š TabTransformer ìµœì¢… ì„±ëŠ¥ ë¦¬í¬íŠ¸]\")\n",
    "print(f\"ì •ë°€ë„(Precision): {precisions[best_idx]:.4f}\")\n",
    "print(f\"ì¬í˜„ìœ¨(Recall): {recalls[best_idx]:.4f}\")\n",
    "print(f\"F1-Score: {f1_scores[best_idx]:.4f}\")\n",
    "print(f\"ROC-AUC ì ìˆ˜: {roc_auc_score(y_test, test_outputs):.4f}\")\n",
    "print(f\"ìµœì  ì„ê³„ê°’: {thresholds[best_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff093acc-214e-4786-b00b-6e2e59a10492",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
