{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca1c70a2-fdcc-4772-b739-f489c5e69b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì…€ 1: ê³µí†µ ì „ì²˜ë¦¬ ì™„ë£Œ (X_train_scaled, X_test_scaled ìƒì„±ë¨)\n"
     ]
    }
   ],
   "source": [
    "# 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import pandas as pd # ë°ì´í„° í•¸ë“¤ë§ìš©\n",
    "import numpy as np # ìˆ˜ì¹˜ ì—°ì‚°ìš©\n",
    "from sklearn.model_selection import train_test_split # ë°ì´í„°ì…‹ ë¶„í• ìš©\n",
    "from sklearn.preprocessing import StandardScaler # ìˆ˜ì¹˜í˜• ë°ì´í„° ìŠ¤ì¼€ì¼ë§ìš©\n",
    "\n",
    "# 2. ì‹ ìš©ì¹´ë“œ ê±°ë˜ ë°ì´í„° ë¡œë“œ (íŒŒì¼ ê²½ë¡œ í™•ì¸ í•„ìš”)\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "\n",
    "# 3. ì‹œê°„(Time) ë°ì´í„°ë¥¼ 24ì‹œê°„ ì£¼ê¸°ì˜ ì‹œê°„ëŒ€(Hour) ì •ë³´ë¡œ ë³€í™˜ (íŒ¨í„´ ì¶”ì¶œ)\n",
    "df['Hour'] = (df['Time'] // 3600) % 24\n",
    "\n",
    "# 4. ê²°ì œ ê¸ˆì•¡(Amount)ì˜ í° ìˆ˜ì¹˜ í¸ì°¨ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë¡œê·¸ ë³€í™˜(Log Transformation) ì ìš©\n",
    "df['Log_Amount'] = np.log1p(df['Amount'])\n",
    "\n",
    "# 5. ëª¨ë¸ í•™ìŠµì— ë¶ˆí•„ìš”í•œ ì›ë³¸ ì»¬ëŸ¼(Time, Amount) ë° ì •ë‹µ(Class) ì œì™¸í•˜ì—¬ í”¼ì²˜ ì„¸íŠ¸ êµ¬ì¶•\n",
    "X = df.drop(['Class', 'Time', 'Amount'], axis=1)\n",
    "\n",
    "# 6. ì •ë‹µ ë ˆì´ë¸”(0: ì •ìƒ, 1: ì‚¬ê¸°)ë§Œ ë³„ë„ë¡œ ë¶„ë¦¬\n",
    "y = df['Class']\n",
    "\n",
    "# 7. í˜„ì‹¤ì ì¸ ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•´ í…ŒìŠ¤íŠ¸ ë°ì´í„°(20%)ë¥¼ ë¨¼ì € ë¶„ë¦¬ (ì‚¬ê¸° ë¹„ìœ¨ ìœ ì§€)\n",
    "# ì´í›„ ì¦ê°• ì‘ì—…ì€ ì˜¤ì§ í•™ìŠµ ë°ì´í„°(X_train, y_train)ì—ë§Œ ìˆ˜í–‰í•¨\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# 8. DL ëª¨ë¸ê³¼ íŠ¸ë¦¬ ëª¨ë¸ ëª¨ë‘ì— ì í•©í•˜ë„ë¡ ë°ì´í„° í‘œì¤€í™”(Scaling) ìˆ˜í–‰\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train) # í•™ìŠµ ë°ì´í„° ê¸°ì¤€ í•™ìŠµ ë° ë³€í™˜\n",
    "X_test_scaled = scaler.transform(X_test) # í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” í•™ìŠµ ê¸°ì¤€ì— ë§ì¶° ë³€í™˜\n",
    "\n",
    "print(\"âœ… ì…€ 1: ê³µí†µ ì „ì²˜ë¦¬ ì™„ë£Œ (X_train_scaled, X_test_scaled ìƒì„±ë¨)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a98d8025-d4d6-44ae-acbd-1debb09733f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì…€ 2: 4ì¢… ë°ì´í„° ì¦ê°• ì™„ë£Œ!\n",
      "- Original: ë°ì´í„° 227845ê±´ (ì‚¬ê¸° ë¹„ì¤‘: 0.2%)\n",
      "- SMOTE: ë°ì´í„° 272941ê±´ (ì‚¬ê¸° ë¹„ì¤‘: 16.7%)\n",
      "- cGAN: ë°ì´í„° 272941ê±´ (ì‚¬ê¸° ë¹„ì¤‘: 16.7%)\n",
      "- K-cGAN: ë°ì´í„° 272935ê±´ (ì‚¬ê¸° ë¹„ì¤‘: 16.7%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunhe\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1. ì¶”ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (ì¦ê°• ë„êµ¬)\n",
    "from imblearn.over_sampling import SMOTE # SMOTE ì¦ê°•ìš©\n",
    "from sklearn.cluster import KMeans # K-cGAN êµ°ì§‘í™”ìš©\n",
    "\n",
    "# 2. ì¦ê°• ëª©í‘œ ë¹„ìœ¨ ì„¤ì • (ì •ìƒ ë°ì´í„°ì˜ 20% ìˆ˜ì¤€ìœ¼ë¡œ ì‚¬ê¸° ë°ì´í„° í™•ë³´)\n",
    "target_ratio = 0.2\n",
    "\n",
    "# --- [ë°©ë²• A] Original: ì¦ê°•í•˜ì§€ ì•Šì€ ë¶ˆê· í˜• ì›ë³¸ í•™ìŠµ ë°ì´í„° ---\n",
    "# ë³€ìˆ˜ëª… í†µì¼ì„ ìœ„í•´ ì›ë³¸ ê·¸ëŒ€ë¡œ ë³µì‚¬\n",
    "X_train_org, y_train_org = X_train_scaled, y_train.values\n",
    "\n",
    "# --- [ë°©ë²• B] SMOTE: ì„ í˜• ë³´ê°„ì„ ì´ìš©í•œ ì •ì„ì ì¸ ì¦ê°• ---\n",
    "smote = SMOTE(sampling_strategy=target_ratio, random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# --- [ë°©ë²• C] cGAN: ì „ì²´ ì‚¬ê¸° ë°ì´í„°ì˜ í†µê³„ê°’(í‰ê· /í¸ì°¨)ì„ ì´ìš©í•œ ë‹¨ìˆœ ìƒì„± ---\n",
    "# ì‹¤ì œ ì‚¬ê¸° ë°ì´í„°ì˜ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°\n",
    "fraud_indices = np.where(y_train == 1)[0]\n",
    "fraud_mean = X_train_scaled[fraud_indices].mean(axis=0)\n",
    "fraud_std = X_train_scaled[fraud_indices].std(axis=0)\n",
    "# ìƒì„±í•´ì•¼ í•  ê°œìˆ˜ ì‚°ì¶œ (ì •ìƒì˜ 20% - í˜„ì¬ ì‚¬ê¸° ìˆ˜)\n",
    "needed_cgan = int(len(X_train_scaled[y_train == 0]) * target_ratio) - len(fraud_indices)\n",
    "# ì •ê·œë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì§œ ì‚¬ê¸° ë°ì´í„° ìƒì„±\n",
    "fake_cgan = np.random.normal(fraud_mean, fraud_std * 0.25, size=(needed_cgan, X_train_scaled.shape[1]))\n",
    "# ì›ë³¸ê³¼ ê²°í•©\n",
    "X_train_cgan = np.vstack([X_train_scaled, fake_cgan])\n",
    "y_train_cgan = np.append(y_train.values, np.ones(needed_cgan))\n",
    "\n",
    "# --- [ë°©ë²• D] K-cGAN: êµ°ì§‘í™”(K-Means) í›„ ê° êµ°ì§‘ë³„ ê°œë³„ ìƒì„± (ê°€ì¥ ì •êµí•¨) ---\n",
    "X_fraud_raw = X_train_scaled[fraud_indices]\n",
    "# ì‚¬ê¸° íŒ¨í„´ì„ 10ê°œë¡œ ìª¼ê°œì–´ í•™ìŠµ (ì„¸ë¶€ íŠ¹ì§• ë³´ì¡´)\n",
    "kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_fraud_raw)\n",
    "needed_kcgan = int(len(X_train_scaled[y_train == 0]) * target_ratio) - len(fraud_indices)\n",
    "gen_per_cluster = needed_kcgan // 10\n",
    "gen_samples_kcgan = []\n",
    "for i in range(10): # ê° ì‚¬ê¸° ê·¸ë£¹ë³„ë¡œ ë°˜ë³µ ìƒì„±\n",
    "    cluster_subset = X_fraud_raw[clusters == i]\n",
    "    fake_subset = np.random.normal(cluster_subset.mean(axis=0), cluster_subset.std(axis=0) * 0.25, size=(gen_per_cluster, X_train_scaled.shape[1]))\n",
    "    gen_samples_kcgan.append(fake_subset)\n",
    "# ìµœì¢… K-cGAN ë°ì´í„°ì…‹ ê²°í•©\n",
    "X_train_kcgan = np.vstack([X_train_scaled, np.vstack(gen_samples_kcgan)])\n",
    "y_train_kcgan = np.append(y_train.values, np.ones(len(np.vstack(gen_samples_kcgan))))\n",
    "\n",
    "# 3. ëª¨ë“  ë°ì´í„°ì…‹ì„ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥í•˜ì—¬ ë¹„êµ ì‹¤í—˜ ì¤€ë¹„ ì™„ë£Œ\n",
    "experimental_sets = {\n",
    "    \"Original\": (X_train_org, y_train_org),\n",
    "    \"SMOTE\": (X_train_smote, y_train_smote),\n",
    "    \"cGAN\": (X_train_cgan, y_train_cgan),\n",
    "    \"K-cGAN\": (X_train_kcgan, y_train_kcgan)\n",
    "}\n",
    "\n",
    "print(f\"âœ… ì…€ 2: 4ì¢… ë°ì´í„° ì¦ê°• ì™„ë£Œ!\")\n",
    "for name, (X_tr, y_tr) in experimental_sets.items():\n",
    "    print(f\"- {name}: ë°ì´í„° {len(X_tr)}ê±´ (ì‚¬ê¸° ë¹„ì¤‘: {np.mean(y_tr)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73bde5e0-7c0d-40f4-ac21-1da30eacb1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Original ë°ì´í„°ì…‹ìœ¼ë¡œ ëœë¤í¬ë ˆìŠ¤íŠ¸ í•™ìŠµ ì¤‘...\n",
      "ğŸš€ SMOTE ë°ì´í„°ì…‹ìœ¼ë¡œ ëœë¤í¬ë ˆìŠ¤íŠ¸ í•™ìŠµ ì¤‘...\n",
      "ğŸš€ cGAN ë°ì´í„°ì…‹ìœ¼ë¡œ ëœë¤í¬ë ˆìŠ¤íŠ¸ í•™ìŠµ ì¤‘...\n",
      "ğŸš€ K-cGAN ë°ì´í„°ì…‹ìœ¼ë¡œ ëœë¤í¬ë ˆìŠ¤íŠ¸ í•™ìŠµ ì¤‘...\n",
      "\n",
      "[ğŸ† ëœë¤í¬ë ˆìŠ¤íŠ¸ ë°ì´í„°ì…‹ë³„ ìµœì¢… ì„±ëŠ¥ ë¹„êµ]\n",
      " Dataset  F1-Score   Recall  Precision  ROC-AUC  Best_Threshold\n",
      "    cGAN  0.894737 0.867347   0.923913 0.960259           0.404\n",
      "  K-cGAN  0.892473 0.846939   0.943182 0.967824           0.440\n",
      "Original  0.888889 0.857143   0.923077 0.961150           0.376\n",
      "   SMOTE  0.877005 0.836735   0.921348 0.978849           0.616\n"
     ]
    }
   ],
   "source": [
    "# 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (ëª¨ë¸ë§ ë° í‰ê°€ìš©)\n",
    "from sklearn.ensemble import RandomForestClassifier # ëœë¤í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜ê¸°\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, roc_auc_score, precision_recall_curve # í‰ê°€ì§€í‘œë“¤\n",
    "import pandas as pd # ê²°ê³¼ ì •ë¦¬ìš©\n",
    "\n",
    "# 2. ê° ì¦ê°• ê¸°ë²•ë³„ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "rf_comparison_results = []\n",
    "\n",
    "# 3. 4ì¢… ë°ì´í„°ì…‹(Original, SMOTE, cGAN, K-cGAN)ì„ í•˜ë‚˜ì”© êº¼ë‚´ì–´ ì‹¤í—˜ ë°˜ë³µ\n",
    "for name, (X_tr, y_tr) in experimental_sets.items():\n",
    "    print(f\"ğŸš€ {name} ë°ì´í„°ì…‹ìœ¼ë¡œ ëœë¤í¬ë ˆìŠ¤íŠ¸ í•™ìŠµ ì¤‘...\")\n",
    "    \n",
    "    # 4. ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ ì„¤ì • (ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ ë‚˜ë¬´ 500ê°œ ì‚¬ìš©)\n",
    "    # n_jobs=-1ì€ ëª¨ë“  CPU ì½”ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—°ì‚° ì†ë„ë¥¼ ë†’ì„\n",
    "    rf_model = RandomForestClassifier(n_estimators=500, max_features='sqrt', n_jobs=-1, random_state=42)\n",
    "    \n",
    "    # 5. í•´ë‹¹ ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ í•™ìŠµ ìˆ˜í–‰\n",
    "    rf_model.fit(X_tr, y_tr)\n",
    "    \n",
    "    # 6. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ ì‚¬ê¸°(Class 1)ì¼ í™•ë¥  ì˜ˆì¸¡\n",
    "    y_probs = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # 7. ì •ë°€ë„-ì¬í˜„ìœ¨ ê³¡ì„ (PR Curve)ì„ í†µí•´ ëª¨ë“  ì„ê³„ê°’ì—ì„œì˜ ì§€í‘œ ê³„ì‚°\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, y_probs)\n",
    "    \n",
    "    # 8. F1-Scoreë¥¼ ìµœëŒ€ë¡œ ë§Œë“œëŠ” ìµœì ì˜ ì„ê³„ê°’(Threshold) ì°¾ê¸°\n",
    "    # f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10) # 0 ë‚˜ëˆ„ê¸° ë°©ì§€\n",
    "    best_idx = np.argmax(f1_scores) # F1ì´ ê°€ì¥ ë†’ì€ ì¸ë±ìŠ¤ ì¶”ì¶œ\n",
    "    best_th = thresholds[best_idx] # í•´ë‹¹ ì§€ì ì˜ ì„ê³„ê°’ ì €ì¥\n",
    "    \n",
    "    # 9. ìµœì  ì„ê³„ê°’ì„ ì ìš©í•œ ìµœì¢… ì§€í‘œ ì‚°ì¶œ ë° ê²°ê³¼ ì €ì¥\n",
    "    rf_comparison_results.append({\n",
    "        \"Dataset\": name,\n",
    "        \"F1-Score\": f1_scores[best_idx],\n",
    "        \"Recall\": recalls[best_idx],\n",
    "        \"Precision\": precisions[best_idx],\n",
    "        \"ROC-AUC\": roc_auc_score(y_test, y_probs),\n",
    "        \"Best_Threshold\": best_th\n",
    "    })\n",
    "\n",
    "# 10. ì „ì²´ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í•œëˆˆì— ë¹„êµ\n",
    "rf_results_df = pd.DataFrame(rf_comparison_results)\n",
    "print(\"\\n[ğŸ† ëœë¤í¬ë ˆìŠ¤íŠ¸ ë°ì´í„°ì…‹ë³„ ìµœì¢… ì„±ëŠ¥ ë¹„êµ]\")\n",
    "print(rf_results_df.sort_values(by=\"F1-Score\", ascending=False).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c35d23cc-06f2-4201-864f-c89bbdcb4d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Preprocessing Raw Data...\n",
      "âœ… Success! Saved to ./data_pipeline/base_preprocessed.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "DATA_PATH = \"./data_pipeline/\"\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)\n",
    "\n",
    "def preprocess_raw_data():\n",
    "    print(\"Step 0: Preprocessing Raw Data...\")\n",
    "    # íŒŒì¼ì´ ê°™ì€ ê²½ë¡œì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”\n",
    "    try:\n",
    "        df = pd.read_csv('creditcard.csv')\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'creditcard.csv' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "        return\n",
    "\n",
    "    # 1. 'Time' ì»¬ëŸ¼ ë³€í™˜ (ì´ˆ -> ì‹œê°„)\n",
    "    df['Time'] = (df['Time'] // 3600) % 24\n",
    "    \n",
    "    # 2. í”¼ì²˜ì™€ ë ˆì´ë¸” ë¶„ë¦¬\n",
    "    X = df.drop('Class', axis=1)\n",
    "    y = df['Class']\n",
    "    \n",
    "    # 3. ì „ì²´ ë°ì´í„° ìŠ¤ì¼€ì¼ë§\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # 4. ì „ì²˜ë¦¬ ì™„ë£Œëœ ë°ì´í„° ì €ì¥\n",
    "    df_preprocessed = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "    df_preprocessed['Class'] = y.values\n",
    "    \n",
    "    output_file = os.path.join(DATA_PATH, \"base_preprocessed.csv\")\n",
    "    df_preprocessed.to_csv(output_file, index=False)\n",
    "    print(f\"âœ… Success! Saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess_raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "245e4c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data...\n",
      "Applying SMOTE (1:1 Ratio)...\n",
      "Total samples after SMOTE: 568630\n",
      "Splitting data 8:2...\n",
      "Saved: ./data_pipeline/train_smote.csv ((454904, 31))\n",
      "Saved: ./data_pipeline/test_smote.csv ((113726, 31))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Airflow ë°ì´í„° ì €ì¥ ê²½ë¡œ\n",
    "DATA_PATH = \"./data_pipeline/\"\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "\n",
    "def run_smote_csv_pipeline():\n",
    "    # 1. ë°ì´í„° ë¡œë“œ (Step 0ì—ì„œ ë§Œë“  ì „ì²˜ë¦¬ íŒŒì¼)\n",
    "    print(\"Loading preprocessed data...\")\n",
    "    # íŒŒì¼ ì½ê¸° ê²°ê³¼ë¥¼ ë³€ìˆ˜ì— í• ë‹¹í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "    df = pd.read_csv(f\"{DATA_PATH}base_preprocessed.csv\")\n",
    "\n",
    "    # 2. X, y ë¶„ë¦¬ (ì¦ê°•ì„ ìœ„í•´ í•„ìš”)\n",
    "    X = df.drop('Class', axis=1)\n",
    "    y = df['Class']\n",
    "\n",
    "    # 3. SMOTE 1:1 ì¦ê°•\n",
    "    print(\"Applying SMOTE (1:1 Ratio)...\")\n",
    "    smote = SMOTE(sampling_strategy=1.0, random_state=42)\n",
    "    # X_scaled_df ëŒ€ì‹  ë¡œë“œí•œ Xë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    \n",
    "    # ì¦ê°•ëœ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ í•©ì¹¨\n",
    "    df_resampled = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "    print(f\"Total samples after SMOTE: {len(df_resampled)}\")\n",
    "\n",
    "    # 4. Train/Test 8:2 ë¶„í• \n",
    "    print(\"Splitting data 8:2...\")\n",
    "    train_df, test_df = train_test_split(\n",
    "        df_resampled, \n",
    "        test_size=0.2, \n",
    "        random_state=42, \n",
    "        stratify=df_resampled['Class']\n",
    "    )\n",
    "\n",
    "    # 5. CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "    train_df.to_csv(f\"{DATA_PATH}train_smote.csv\", index=False)\n",
    "    test_df.to_csv(f\"{DATA_PATH}test_smote.csv\", index=False)\n",
    "    \n",
    "    print(f\"Saved: {DATA_PATH}train_smote.csv ({train_df.shape})\")\n",
    "    print(f\"Saved: {DATA_PATH}test_smote.csv ({test_df.shape})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_smote_csv_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f532e823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading Data...\n",
      "Using device: cpu\n",
      "Step 2: Training cGAN (100 Epochs)...\n",
      "Epoch [20/100] D Loss: 0.0654, G Loss: 4.9797\n",
      "Epoch [40/100] D Loss: 0.0110, G Loss: 8.0226\n",
      "Epoch [60/100] D Loss: 0.0186, G Loss: 9.7550\n",
      "Epoch [80/100] D Loss: 0.0018, G Loss: 10.2790\n",
      "Epoch [100/100] D Loss: 0.0085, G Loss: 8.6982\n",
      "Step 3: Generating Data and Splitting...\n",
      "âœ… Success! Saved to ./data_pipeline/train_cgan.csv and test_cgan.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. ê²½ë¡œ ì„¤ì • (ë°˜ë“œì‹œ í´ë”ê°€ ì¡´ì¬í•´ì•¼ í•¨)\n",
    "DATA_PATH = \"./data_pipeline/\"\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs(DATA_PATH)\n",
    "\n",
    "# 2. ëª¨ë¸ ì •ì˜\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, cond_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim + cond_dim, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise, labels):\n",
    "        # [ì˜¤íƒ€ ìˆ˜ì •] 1a -> 1\n",
    "        x = torch.cat([noise, labels], 1)\n",
    "        return self.model(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, data_dim, cond_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(data_dim + cond_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, data, labels):\n",
    "        x = torch.cat([data, labels], 1)\n",
    "        return self.model(x)\n",
    "\n",
    "# 3. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\n",
    "def run_cgan_csv_pipeline():\n",
    "    print(\"Step 1: Loading Data...\")\n",
    "    try:\n",
    "        df = pd.read_csv(f\"{DATA_PATH}base_preprocessed.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {DATA_PATH}base_preprocessed.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì „ì²˜ë¦¬ë¥¼ ë¨¼ì € ìˆ˜í–‰í•˜ì„¸ìš”.\")\n",
    "        return\n",
    "\n",
    "    X = df.drop('Class', axis=1)\n",
    "    y = df['Class']\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    latent_dim, data_dim, cond_dim = 100, X.shape[1], 1\n",
    "    G = Generator(latent_dim, cond_dim, data_dim).to(device)\n",
    "    D = Discriminator(data_dim, cond_dim).to(device)\n",
    "    optim_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optim_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    X_tensor = torch.FloatTensor(X.values).to(device)\n",
    "    y_tensor = torch.FloatTensor(y.values).view(-1, 1).to(device)\n",
    "    loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_tensor, y_tensor), batch_size=1024, shuffle=True)\n",
    "\n",
    "    print(\"Step 2: Training cGAN (100 Epochs)...\")\n",
    "    for epoch in range(100):\n",
    "        for real_data, labels in loader:\n",
    "            b_size = real_data.size(0)\n",
    "            \n",
    "            # Discriminator í•™ìŠµ\n",
    "            optim_D.zero_grad()\n",
    "            real_loss = criterion(D(real_data, labels), torch.ones(b_size, 1).to(device))\n",
    "            z = torch.randn(b_size, latent_dim).to(device)\n",
    "            fake_data = G(z, labels)\n",
    "            fake_loss = criterion(D(fake_data.detach(), labels), torch.zeros(b_size, 1).to(device))\n",
    "            d_loss = real_loss + fake_loss\n",
    "            d_loss.backward()\n",
    "            optim_D.step()\n",
    "\n",
    "            # Generator í•™ìŠµ\n",
    "            optim_G.zero_grad()\n",
    "            g_loss = criterion(D(fake_data, labels), torch.ones(b_size, 1).to(device))\n",
    "            g_loss.backward()\n",
    "            optim_G.step()\n",
    "        \n",
    "        if (epoch+1) % 20 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/100] D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "    print(\"Step 3: Generating Data and Splitting...\")\n",
    "    num_gen = sum(y == 0) - sum(y == 1)\n",
    "    z = torch.randn(num_gen, latent_dim).to(device)\n",
    "    cond = torch.ones(num_gen, 1).to(device)\n",
    "    \n",
    "    G.eval()\n",
    "    with torch.no_grad():\n",
    "        gen_samples = G(z, cond).cpu().numpy()\n",
    "    \n",
    "    df_gen = pd.DataFrame(gen_samples, columns=X.columns)\n",
    "    df_gen['Class'] = 1\n",
    "    df_final = pd.concat([df, df_gen], axis=0)\n",
    "\n",
    "    train_df, test_df = train_test_split(df_final, test_size=0.2, random_state=42, stratify=df_final['Class'])\n",
    "\n",
    "    # íŒŒì¼ ì €ì¥ ë¶€ë¶„\n",
    "    train_df.to_csv(f\"{DATA_PATH}train_cgan.csv\", index=False)\n",
    "    test_df.to_csv(f\"{DATA_PATH}test_cgan.csv\", index=False)\n",
    "    print(f\"âœ… Success! Saved to {DATA_PATH}train_cgan.csv and test_cgan.csv\")\n",
    "\n",
    "# âš ï¸ ì´ ë¶€ë¶„ì´ ì—†ìœ¼ë©´ í•¨ìˆ˜ê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤!\n",
    "if __name__ == \"__main__\":\n",
    "    run_cgan_csv_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5218a50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •\n",
    "DATA_PATH = \"./data_pipeline/\"\n",
    "os.makedirs(DATA_PATH, exist_ok=True)\n",
    "\n",
    "# 1. ëª¨ë¸ ì •ì˜\n",
    "class K_Generator(nn.Module):\n",
    "    def __init__(self, input_dim, cond_dim, output_dim):\n",
    "        super(K_Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim + cond_dim, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, noise, cond):\n",
    "        x = torch.cat([noise, cond], 1)\n",
    "        return self.model(x)\n",
    "\n",
    "class K_Discriminator(nn.Module):\n",
    "    def __init__(self, data_dim, cond_dim):\n",
    "        super(K_Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(data_dim + cond_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, data, cond):\n",
    "        x = torch.cat([data, cond], 1)\n",
    "        return self.model(x)\n",
    "\n",
    "# 2. ë©”ì¸ íŒŒì´í”„ë¼ì¸\n",
    "def run_kcgan_csv_pipeline():\n",
    "    # 1. ë°ì´í„° ë¡œë“œ\n",
    "    print(\"Loading preprocessed data...\")\n",
    "    if not os.path.exists(f\"{DATA_PATH}base_preprocessed.csv\"):\n",
    "        print(\"Error: base_preprocessed.csvê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "    \n",
    "    df = pd.read_csv(f\"{DATA_PATH}base_preprocessed.csv\")\n",
    "    X_all = df.drop('Class', axis=1)\n",
    "    y_all = df['Class']\n",
    "\n",
    "    # 2. K-means Clustering\n",
    "    print(\"Clustering Fraud Data...\")\n",
    "    n_clusters = 5\n",
    "    fraud_mask = (y_all == 1)\n",
    "    X_fraud = X_all[fraud_mask].values\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    fraud_clusters = kmeans.fit_predict(X_fraud)\n",
    "    \n",
    "    cluster_labels = np.zeros(len(df))\n",
    "    cluster_labels[fraud_mask] = fraud_clusters + 1 \n",
    "    \n",
    "    # One-hot Encoding\n",
    "    cond_matrix = pd.get_dummies(cluster_labels).values\n",
    "    cond_dim = cond_matrix.shape[1]\n",
    "    \n",
    "    # 3. ëª¨ë¸ ì´ˆê¸°í™”\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    latent_dim, data_dim = 100, X_all.shape[1]\n",
    "    G = K_Generator(latent_dim, cond_dim, data_dim).to(device)\n",
    "    D = K_Discriminator(data_dim, cond_dim).to(device)\n",
    "    \n",
    "    optimizer_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    X_tensor = torch.FloatTensor(X_all.values).to(device)\n",
    "    C_tensor = torch.FloatTensor(cond_matrix).to(device)\n",
    "    loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_tensor, C_tensor), batch_size=1024, shuffle=True)\n",
    "\n",
    "    # 4. í•™ìŠµ ë£¨í”„\n",
    "    print(f\"Training K-cGAN on {device}...\")\n",
    "    G.train(); D.train()\n",
    "    for epoch in range(50):\n",
    "        for real_data, conds in loader:\n",
    "            b_size = real_data.size(0)\n",
    "            \n",
    "            # Discriminator í•™ìŠµ\n",
    "            optimizer_D.zero_grad()\n",
    "            real_out = D(real_data, conds)\n",
    "            d_loss_real = criterion(real_out, torch.ones(b_size, 1).to(device))\n",
    "            \n",
    "            z = torch.randn(b_size, latent_dim).to(device)\n",
    "            fake_data = G(z, conds)\n",
    "            fake_out = D(fake_data.detach(), conds)\n",
    "            d_loss_fake = criterion(fake_out, torch.zeros(b_size, 1).to(device))\n",
    "            \n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            d_loss.backward(); optimizer_D.step()\n",
    "\n",
    "            # Generator í•™ìŠµ\n",
    "            optimizer_G.zero_grad()\n",
    "            g_out = D(fake_data, conds)\n",
    "            g_loss = criterion(g_out, torch.ones(b_size, 1).to(device))\n",
    "            g_loss.backward(); optimizer_G.step()\n",
    "        \n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/50] D_loss: {d_loss.item():.4f}\")\n",
    "\n",
    "    # 5. ì‚¬ê¸° ë°ì´í„° ìƒì„± ë° ë¶„í• \n",
    "    print(\"Generating Synthetic Data...\")\n",
    "    num_to_gen = sum(y_all == 0) - sum(y_all == 1)\n",
    "    \n",
    "    # êµ°ì§‘ ì¡°ê±´ì„ ëœë¤í•˜ê²Œ ìƒì„± (ì‚¬ê¸° êµ°ì§‘ 1~5ë²ˆ ì¤‘ í•˜ë‚˜ ì„ íƒ)\n",
    "    gen_cluster_ids = np.random.randint(1, n_clusters + 1, num_to_gen)\n",
    "    \n",
    "    # âš ï¸ ì¤‘ìš”: One-hot Matrix í¬ê¸°ë¥¼ ì›ë³¸ í•™ìŠµ ì‹œì™€ ë™ì¼í•˜ê²Œ(cond_dim) ë§ì¶¤\n",
    "    gen_cond = np.zeros((num_to_gen, cond_dim))\n",
    "    for i, cid in enumerate(gen_cluster_ids):\n",
    "        gen_cond[i, cid] = 1.0\n",
    "    \n",
    "    z = torch.randn(num_to_gen, latent_dim).to(device)\n",
    "    G.eval()\n",
    "    with torch.no_grad():\n",
    "        gen_samples = G(z, torch.FloatTensor(gen_cond).to(device)).cpu().numpy()\n",
    "\n",
    "    # 6. ìµœì¢… í•©ì¹˜ê¸° ë° 8:2 ë¶„í• \n",
    "    df_gen = pd.DataFrame(gen_samples, columns=X_all.columns)\n",
    "    df_gen['Class'] = 1\n",
    "    df_final = pd.concat([df, df_gen], axis=0)\n",
    "\n",
    "    train_df, test_df = train_test_split(df_final, test_size=0.2, random_state=42, stratify=df_final['Class'])\n",
    "\n",
    "    # 7. CSV ì €ì¥ (í˜¸ì¶œì´ ì„±ê³µí•˜ë©´ ì•„ë˜ ë¡œê·¸ê°€ ì°í™ë‹ˆë‹¤)\n",
    "    train_df.to_csv(f\"{DATA_PATH}train_kcgan.csv\", index=False)\n",
    "    test_df.to_csv(f\"{DATA_PATH}test_kcgan.csv\", index=False)\n",
    "    print(f\"âœ… Success! K-cGAN CSV Saved to {DATA_PATH}\")\n",
    "\n",
    "# ğŸš€ ì´ ë¶€ë¶„ì´ ìˆì–´ì•¼ ì‹¤ì œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤!\n",
    "if __name__ == \"__main__\":\n",
    "    run_kcgan_csv_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcf9ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, regularizers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ==========================================\n",
    "# 1. ë°ì´í„° ì „ì²˜ë¦¬ (Data Preprocessing)\n",
    "# ==========================================\n",
    "def load_data(filepath='creditcard.csv'):\n",
    "    # ë°ì´í„° ë¡œë“œ \n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Timeê³¼ Amount ì •ê·œí™” (ë…¼ë¬¸ì—ì„œëŠ” Time, AmountëŠ” PCAë˜ì§€ ì•ŠìŒ) \n",
    "    df['Amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1, 1))\n",
    "    df['Time'] = StandardScaler().fit_transform(df['Time'].values.reshape(-1, 1))\n",
    "    \n",
    "    # í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°ì„ ìœ„í•´ ì†Œìˆ˜ í´ë˜ìŠ¤(ì‚¬ê¸°, Class=1)ë§Œ ì¶”ì¶œí•˜ì—¬ í•™ìŠµ\n",
    "    # GANì˜ ëª©ì ì€ ì‚¬ê¸° ë°ì´í„°ë¥¼ ì¦ê°•í•˜ëŠ” ê²ƒì„ [cite: 10, 290]\n",
    "    fraud_df = df[df['Class'] == 1]\n",
    "    \n",
    "    # Featureì™€ Label ë¶„ë¦¬\n",
    "    X = fraud_df.drop('Class', axis=1).values\n",
    "    y = fraud_df['Class'].values.reshape(-1, 1)\n",
    "    \n",
    "    return X, y, df.shape[1] - 1  # -1ì€ Class ì»¬ëŸ¼ ì œì™¸\n",
    "\n",
    "# ==========================================\n",
    "# 2. ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ì˜ (Table I & II)\n",
    "# ==========================================\n",
    "\n",
    "def build_generator(input_dim, num_classes=2):\n",
    "    # Hyperparameters from Table I \n",
    "    noise_dim = 100\n",
    "    dropout_rate = 0.1\n",
    "    l2_reg = regularizers.l2(0.01) # L2 method used\n",
    "    \n",
    "    # Inputs\n",
    "    noise_input = layers.Input(shape=(noise_dim,))\n",
    "    label_input = layers.Input(shape=(1,), dtype='int32')\n",
    "    \n",
    "    # Label Embedding (Conditional GAN)\n",
    "    label_embedding = layers.Embedding(num_classes, noise_dim)(label_input)\n",
    "    label_embedding = layers.Flatten()(label_embedding)\n",
    "    \n",
    "    # Concatenate Noise + Label\n",
    "    merged = layers.Concatenate()([noise_input, label_embedding])\n",
    "    \n",
    "    # Hidden Layers (128, 64 as per Table I)\n",
    "    x = layers.Dense(128, activation='relu', kernel_regularizer=l2_reg)(merged)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = layers.Dense(64, activation='relu', kernel_regularizer=l2_reg)(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Output Layer (Feature dimension)\n",
    "    # ë°ì´í„°ê°€ StandardScaling ë˜ì—ˆìœ¼ë¯€ë¡œ Linear í˜¹ì€ Tanh ì‚¬ìš© (ì—¬ê¸°ì„  Linear ì‚¬ìš©)\n",
    "    out = layers.Dense(input_dim, activation='linear')(x)\n",
    "    \n",
    "    model = models.Model([noise_input, label_input], out, name=\"Generator\")\n",
    "    return model\n",
    "\n",
    "def build_discriminator(input_dim, num_classes=2):\n",
    "    # Hyperparameters from Table II \n",
    "    dropout_rate = 0.1\n",
    "    l2_reg = regularizers.l2(0.01)\n",
    "    \n",
    "    # Inputs\n",
    "    img_input = layers.Input(shape=(input_dim,))\n",
    "    label_input = layers.Input(shape=(1,), dtype='int32')\n",
    "    \n",
    "    # Label Embedding\n",
    "    label_embedding = layers.Embedding(num_classes, input_dim)(label_input)\n",
    "    label_embedding = layers.Flatten()(label_embedding) # (Batch, input_dim)\n",
    "    \n",
    "    # Concatenate Data + Label\n",
    "    merged = layers.Concatenate()([img_input, label_embedding])\n",
    "    \n",
    "    # Hidden Layers (20, 10 as per Table II - 2 layers)\n",
    "    x = layers.Dense(20, kernel_regularizer=l2_reg)(merged)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x) # LeakyRelu [cite: 164]\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = layers.Dense(10, kernel_regularizer=l2_reg)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Output\n",
    "    out = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = models.Model([img_input, label_input], out, name=\"Discriminator\")\n",
    "    return model\n",
    "\n",
    "# ==========================================\n",
    "# 3. K-CGAN í´ë˜ìŠ¤ ë° Custom Loss (í•µì‹¬)\n",
    "# ==========================================\n",
    "\n",
    "class K_CGAN(tf.keras.Model):\n",
    "    def __init__(self, generator, discriminator, noise_dim=100):\n",
    "        super(K_CGAN, self).__init__()\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.noise_dim = noise_dim\n",
    "        \n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(K_CGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn # Binary Cross Entropy\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack data (real_features, real_labels)\n",
    "        real_images, labels = data\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        \n",
    "        # 1. Train Discriminator\n",
    "        noise = tf.random.normal([batch_size, self.noise_dim])\n",
    "        fake_images = self.generator([noise, labels], training=True)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            real_output = self.discriminator([real_images, labels], training=True)\n",
    "            fake_output = self.discriminator([fake_images, labels], training=True)\n",
    "            \n",
    "            d_loss_real = self.loss_fn(tf.ones_like(real_output), real_output)\n",
    "            d_loss_fake = self.loss_fn(tf.zeros_like(fake_output), fake_output)\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            \n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "        self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_variables))\n",
    "\n",
    "        # 2. Train Generator (with KL Divergence)\n",
    "        noise = tf.random.normal([batch_size, self.noise_dim])\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_images = self.generator([noise, labels], training=True)\n",
    "            fake_output = self.discriminator([fake_images, labels], training=True)\n",
    "            \n",
    "            # Generator Objective 1: Fool Discriminator (Binary Cross Entropy) [cite: 132]\n",
    "            g_loss_bce = self.loss_fn(tf.ones_like(fake_output), fake_output)\n",
    "            \n",
    "            # Generator Objective 2: KL Divergence \n",
    "            # ë¶„í¬ ê°„ì˜ ì°¨ì´ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ë°°ì¹˜ ë‚´ í†µê³„ëŸ‰(í‰ê· , ë¶„ì‚°)ì„ ì´ìš©í•œ Gaussian KL ê·¼ì‚¬ ì‚¬ìš©\n",
    "            # KL(P || Q) = log(std_q/std_p) + (std_p^2 + (mu_p - mu_q)^2) / (2*std_q^2) - 0.5\n",
    "            \n",
    "            mu_real = tf.reduce_mean(real_images, axis=0)\n",
    "            std_real = tf.math.reduce_std(real_images, axis=0) + 1e-6\n",
    "            \n",
    "            mu_fake = tf.reduce_mean(fake_images, axis=0)\n",
    "            std_fake = tf.math.reduce_std(fake_images, axis=0) + 1e-6\n",
    "            \n",
    "            kl_loss = tf.reduce_sum(\n",
    "                tf.math.log(std_fake / std_real) + \n",
    "                ((tf.square(std_real) + tf.square(mu_real - mu_fake)) / (2 * tf.square(std_fake))) - \n",
    "                0.5\n",
    "            )\n",
    "            \n",
    "            # Total Generator Loss (Equation 3 ìœ ì‚¬ êµ¬í˜„)\n",
    "            # ë…¼ë¬¸ì—ì„œëŠ” KL í•­ì„ ì¶”ê°€í–ˆë‹¤ê³  ëª…ì‹œ \n",
    "            g_loss = g_loss_bce + (0.1 * kl_loss) # 0.1 is a weighting factor (tunable)\n",
    "\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_variables))\n",
    "        \n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss, \"kl_loss\": kl_loss}\n",
    "\n",
    "# ==========================================\n",
    "# 4. ì‹¤í–‰ ì½”ë“œ (Execution)\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Settings\n",
    "    EPOCHS = 100         # [cite: 143]\n",
    "    BATCH_SIZE = 64      # [cite: 143]\n",
    "    LR = 0.0001          # \n",
    "    \n",
    "    try:\n",
    "        # 1. ë°ì´í„° ë¡œë“œ\n",
    "        X_train, y_train, input_dim = load_data('creditcard.csv')\n",
    "        print(f\"Loaded Fraud Data Shape: {X_train.shape}\")\n",
    "        \n",
    "        # 2. ëª¨ë¸ ìƒì„±\n",
    "        d_model = build_discriminator(input_dim)\n",
    "        g_model = build_generator(input_dim)\n",
    "        \n",
    "        # 3. K-CGAN ì¸ìŠ¤í„´ìŠ¤í™”\n",
    "        k_cgan = K_CGAN(generator=g_model, discriminator=d_model)\n",
    "        \n",
    "        # 4. ì»´íŒŒì¼ (Optimizers from Table I, II)\n",
    "        k_cgan.compile(\n",
    "            d_optimizer=optimizers.Adam(learning_rate=LR),\n",
    "            g_optimizer=optimizers.Adam(learning_rate=LR),\n",
    "            loss_fn=tf.keras.losses.BinaryCrossentropy()\n",
    "        )\n",
    "        \n",
    "        # 5. í•™ìŠµ ì‹œì‘\n",
    "        print(\"Starting Training...\")\n",
    "        k_cgan.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS)\n",
    "        \n",
    "        # 6. í•©ì„± ë°ì´í„° ìƒì„± (ì˜ˆì‹œ)\n",
    "        print(\"Generating Synthetic Data...\")\n",
    "        num_synthetic_samples = 100\n",
    "        noise = np.random.normal(0, 1, (num_synthetic_samples, 100))\n",
    "        sampled_labels = np.ones((num_synthetic_samples, 1)) # ì‚¬ê¸° ë°ì´í„°(1) ìƒì„±\n",
    "        \n",
    "        generated_data = g_model.predict([noise, sampled_labels])\n",
    "        print(f\"Generated Data Shape: {generated_data.shape}\")\n",
    "        \n",
    "        # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ì €ì¥ ê°€ëŠ¥\n",
    "        # synthetic_df = pd.DataFrame(generated_data, columns=...)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"ì˜¤ë¥˜: 'creditcard.csv' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê°™ì€ ë””ë ‰í† ë¦¬ì— íŒŒì¼ì„ ìœ„ì¹˜ì‹œì¼œì£¼ì„¸ìš”.\")"
   ]
   "execution_count": null,
   "id": "c35d23cc-06f2-4201-864f-c89bbdcb4d28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
