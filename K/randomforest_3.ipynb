{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca1c70a2-fdcc-4772-b739-f489c5e69b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì…€ 1: ê³µí†µ ì „ì²˜ë¦¬ ì™„ë£Œ (X_train_scaled, X_test_scaled ìƒì„±ë¨)\n"
     ]
    }
   ],
   "source": [
    "# 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import pandas as pd # ë°ì´í„° í•¸ë“¤ë§ìš©\n",
    "import numpy as np # ìˆ˜ì¹˜ ì—°ì‚°ìš©\n",
    "from sklearn.model_selection import train_test_split # ë°ì´í„°ì…‹ ë¶„í• ìš©\n",
    "from sklearn.preprocessing import StandardScaler # ìˆ˜ì¹˜í˜• ë°ì´í„° ìŠ¤ì¼€ì¼ë§ìš©\n",
    "\n",
    "# 2. ì‹ ìš©ì¹´ë“œ ê±°ë˜ ë°ì´í„° ë¡œë“œ (íŒŒì¼ ê²½ë¡œ í™•ì¸ í•„ìš”)\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "\n",
    "# 3. ì‹œê°„(Time) ë°ì´í„°ë¥¼ 24ì‹œê°„ ì£¼ê¸°ì˜ ì‹œê°„ëŒ€(Hour) ì •ë³´ë¡œ ë³€í™˜ (íŒ¨í„´ ì¶”ì¶œ)\n",
    "df['Hour'] = (df['Time'] // 3600) % 24\n",
    "\n",
    "# 4. ê²°ì œ ê¸ˆì•¡(Amount)ì˜ í° ìˆ˜ì¹˜ í¸ì°¨ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë¡œê·¸ ë³€í™˜(Log Transformation) ì ìš©\n",
    "df['Log_Amount'] = np.log1p(df['Amount'])\n",
    "\n",
    "# 5. ëª¨ë¸ í•™ìŠµì— ë¶ˆí•„ìš”í•œ ì›ë³¸ ì»¬ëŸ¼(Time, Amount) ë° ì •ë‹µ(Class) ì œì™¸í•˜ì—¬ í”¼ì²˜ ì„¸íŠ¸ êµ¬ì¶•\n",
    "X = df.drop(['Class', 'Time', 'Amount'], axis=1)\n",
    "\n",
    "# 6. ì •ë‹µ ë ˆì´ë¸”(0: ì •ìƒ, 1: ì‚¬ê¸°)ë§Œ ë³„ë„ë¡œ ë¶„ë¦¬\n",
    "y = df['Class']\n",
    "\n",
    "# 7. í˜„ì‹¤ì ì¸ ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•´ í…ŒìŠ¤íŠ¸ ë°ì´í„°(20%)ë¥¼ ë¨¼ì € ë¶„ë¦¬ (ì‚¬ê¸° ë¹„ìœ¨ ìœ ì§€)\n",
    "# ì´í›„ ì¦ê°• ì‘ì—…ì€ ì˜¤ì§ í•™ìŠµ ë°ì´í„°(X_train, y_train)ì—ë§Œ ìˆ˜í–‰í•¨\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# 8. DL ëª¨ë¸ê³¼ íŠ¸ë¦¬ ëª¨ë¸ ëª¨ë‘ì— ì í•©í•˜ë„ë¡ ë°ì´í„° í‘œì¤€í™”(Scaling) ìˆ˜í–‰\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train) # í•™ìŠµ ë°ì´í„° ê¸°ì¤€ í•™ìŠµ ë° ë³€í™˜\n",
    "X_test_scaled = scaler.transform(X_test) # í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” í•™ìŠµ ê¸°ì¤€ì— ë§ì¶° ë³€í™˜\n",
    "\n",
    "print(\"âœ… ì…€ 1: ê³µí†µ ì „ì²˜ë¦¬ ì™„ë£Œ (X_train_scaled, X_test_scaled ìƒì„±ë¨)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a98d8025-d4d6-44ae-acbd-1debb09733f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì…€ 2: 4ì¢… ë°ì´í„° ì¦ê°• ì™„ë£Œ!\n",
      "- Original: ë°ì´í„° 227845ê±´ (ì‚¬ê¸° ë¹„ì¤‘: 0.2%)\n",
      "- SMOTE: ë°ì´í„° 272941ê±´ (ì‚¬ê¸° ë¹„ì¤‘: 16.7%)\n",
      "- cGAN: ë°ì´í„° 272941ê±´ (ì‚¬ê¸° ë¹„ì¤‘: 16.7%)\n",
      "- K-cGAN: ë°ì´í„° 272935ê±´ (ì‚¬ê¸° ë¹„ì¤‘: 16.7%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sunhe\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1. ì¶”ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (ì¦ê°• ë„êµ¬)\n",
    "from imblearn.over_sampling import SMOTE # SMOTE ì¦ê°•ìš©\n",
    "from sklearn.cluster import KMeans # K-cGAN êµ°ì§‘í™”ìš©\n",
    "\n",
    "# 2. ì¦ê°• ëª©í‘œ ë¹„ìœ¨ ì„¤ì • (ì •ìƒ ë°ì´í„°ì˜ 20% ìˆ˜ì¤€ìœ¼ë¡œ ì‚¬ê¸° ë°ì´í„° í™•ë³´)\n",
    "target_ratio = 0.2\n",
    "\n",
    "# --- [ë°©ë²• A] Original: ì¦ê°•í•˜ì§€ ì•Šì€ ë¶ˆê· í˜• ì›ë³¸ í•™ìŠµ ë°ì´í„° ---\n",
    "# ë³€ìˆ˜ëª… í†µì¼ì„ ìœ„í•´ ì›ë³¸ ê·¸ëŒ€ë¡œ ë³µì‚¬\n",
    "X_train_org, y_train_org = X_train_scaled, y_train.values\n",
    "\n",
    "# --- [ë°©ë²• B] SMOTE: ì„ í˜• ë³´ê°„ì„ ì´ìš©í•œ ì •ì„ì ì¸ ì¦ê°• ---\n",
    "smote = SMOTE(sampling_strategy=target_ratio, random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# --- [ë°©ë²• C] cGAN: ì „ì²´ ì‚¬ê¸° ë°ì´í„°ì˜ í†µê³„ê°’(í‰ê· /í¸ì°¨)ì„ ì´ìš©í•œ ë‹¨ìˆœ ìƒì„± ---\n",
    "# ì‹¤ì œ ì‚¬ê¸° ë°ì´í„°ì˜ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°\n",
    "fraud_indices = np.where(y_train == 1)[0]\n",
    "fraud_mean = X_train_scaled[fraud_indices].mean(axis=0)\n",
    "fraud_std = X_train_scaled[fraud_indices].std(axis=0)\n",
    "# ìƒì„±í•´ì•¼ í•  ê°œìˆ˜ ì‚°ì¶œ (ì •ìƒì˜ 20% - í˜„ì¬ ì‚¬ê¸° ìˆ˜)\n",
    "needed_cgan = int(len(X_train_scaled[y_train == 0]) * target_ratio) - len(fraud_indices)\n",
    "# ì •ê·œë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì§œ ì‚¬ê¸° ë°ì´í„° ìƒì„±\n",
    "fake_cgan = np.random.normal(fraud_mean, fraud_std * 0.25, size=(needed_cgan, X_train_scaled.shape[1]))\n",
    "# ì›ë³¸ê³¼ ê²°í•©\n",
    "X_train_cgan = np.vstack([X_train_scaled, fake_cgan])\n",
    "y_train_cgan = np.append(y_train.values, np.ones(needed_cgan))\n",
    "\n",
    "# --- [ë°©ë²• D] K-cGAN: êµ°ì§‘í™”(K-Means) í›„ ê° êµ°ì§‘ë³„ ê°œë³„ ìƒì„± (ê°€ì¥ ì •êµí•¨) ---\n",
    "X_fraud_raw = X_train_scaled[fraud_indices]\n",
    "# ì‚¬ê¸° íŒ¨í„´ì„ 10ê°œë¡œ ìª¼ê°œì–´ í•™ìŠµ (ì„¸ë¶€ íŠ¹ì§• ë³´ì¡´)\n",
    "kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_fraud_raw)\n",
    "needed_kcgan = int(len(X_train_scaled[y_train == 0]) * target_ratio) - len(fraud_indices)\n",
    "gen_per_cluster = needed_kcgan // 10\n",
    "gen_samples_kcgan = []\n",
    "for i in range(10): # ê° ì‚¬ê¸° ê·¸ë£¹ë³„ë¡œ ë°˜ë³µ ìƒì„±\n",
    "    cluster_subset = X_fraud_raw[clusters == i]\n",
    "    fake_subset = np.random.normal(cluster_subset.mean(axis=0), cluster_subset.std(axis=0) * 0.25, size=(gen_per_cluster, X_train_scaled.shape[1]))\n",
    "    gen_samples_kcgan.append(fake_subset)\n",
    "# ìµœì¢… K-cGAN ë°ì´í„°ì…‹ ê²°í•©\n",
    "X_train_kcgan = np.vstack([X_train_scaled, np.vstack(gen_samples_kcgan)])\n",
    "y_train_kcgan = np.append(y_train.values, np.ones(len(np.vstack(gen_samples_kcgan))))\n",
    "\n",
    "# 3. ëª¨ë“  ë°ì´í„°ì…‹ì„ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥í•˜ì—¬ ë¹„êµ ì‹¤í—˜ ì¤€ë¹„ ì™„ë£Œ\n",
    "experimental_sets = {\n",
    "    \"Original\": (X_train_org, y_train_org),\n",
    "    \"SMOTE\": (X_train_smote, y_train_smote),\n",
    "    \"cGAN\": (X_train_cgan, y_train_cgan),\n",
    "    \"K-cGAN\": (X_train_kcgan, y_train_kcgan)\n",
    "}\n",
    "\n",
    "print(f\"âœ… ì…€ 2: 4ì¢… ë°ì´í„° ì¦ê°• ì™„ë£Œ!\")\n",
    "for name, (X_tr, y_tr) in experimental_sets.items():\n",
    "    print(f\"- {name}: ë°ì´í„° {len(X_tr)}ê±´ (ì‚¬ê¸° ë¹„ì¤‘: {np.mean(y_tr)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73bde5e0-7c0d-40f4-ac21-1da30eacb1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Original ë°ì´í„°ì…‹ìœ¼ë¡œ ëœë¤í¬ë ˆìŠ¤íŠ¸ í•™ìŠµ ì¤‘...\n",
      "ğŸš€ SMOTE ë°ì´í„°ì…‹ìœ¼ë¡œ ëœë¤í¬ë ˆìŠ¤íŠ¸ í•™ìŠµ ì¤‘...\n",
      "ğŸš€ cGAN ë°ì´í„°ì…‹ìœ¼ë¡œ ëœë¤í¬ë ˆìŠ¤íŠ¸ í•™ìŠµ ì¤‘...\n",
      "ğŸš€ K-cGAN ë°ì´í„°ì…‹ìœ¼ë¡œ ëœë¤í¬ë ˆìŠ¤íŠ¸ í•™ìŠµ ì¤‘...\n",
      "\n",
      "[ğŸ† ëœë¤í¬ë ˆìŠ¤íŠ¸ ë°ì´í„°ì…‹ë³„ ìµœì¢… ì„±ëŠ¥ ë¹„êµ]\n",
      " Dataset  F1-Score   Recall  Precision  ROC-AUC  Best_Threshold\n",
      "    cGAN  0.894737 0.867347   0.923913 0.960259           0.404\n",
      "  K-cGAN  0.892473 0.846939   0.943182 0.967824           0.440\n",
      "Original  0.888889 0.857143   0.923077 0.961150           0.376\n",
      "   SMOTE  0.877005 0.836735   0.921348 0.978849           0.616\n"
     ]
    }
   ],
   "source": [
    "# 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (ëª¨ë¸ë§ ë° í‰ê°€ìš©)\n",
    "from sklearn.ensemble import RandomForestClassifier # ëœë¤í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜ê¸°\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, roc_auc_score, precision_recall_curve # í‰ê°€ì§€í‘œë“¤\n",
    "import pandas as pd # ê²°ê³¼ ì •ë¦¬ìš©\n",
    "\n",
    "# 2. ê° ì¦ê°• ê¸°ë²•ë³„ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”\n",
    "rf_comparison_results = []\n",
    "\n",
    "# 3. 4ì¢… ë°ì´í„°ì…‹(Original, SMOTE, cGAN, K-cGAN)ì„ í•˜ë‚˜ì”© êº¼ë‚´ì–´ ì‹¤í—˜ ë°˜ë³µ\n",
    "for name, (X_tr, y_tr) in experimental_sets.items():\n",
    "    print(f\"ğŸš€ {name} ë°ì´í„°ì…‹ìœ¼ë¡œ ëœë¤í¬ë ˆìŠ¤íŠ¸ í•™ìŠµ ì¤‘...\")\n",
    "    \n",
    "    # 4. ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ ì„¤ì • (ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•´ ë‚˜ë¬´ 500ê°œ ì‚¬ìš©)\n",
    "    # n_jobs=-1ì€ ëª¨ë“  CPU ì½”ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—°ì‚° ì†ë„ë¥¼ ë†’ì„\n",
    "    rf_model = RandomForestClassifier(n_estimators=500, max_features='sqrt', n_jobs=-1, random_state=42)\n",
    "    \n",
    "    # 5. í•´ë‹¹ ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ í•™ìŠµ ìˆ˜í–‰\n",
    "    rf_model.fit(X_tr, y_tr)\n",
    "    \n",
    "    # 6. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ ì‚¬ê¸°(Class 1)ì¼ í™•ë¥  ì˜ˆì¸¡\n",
    "    y_probs = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # 7. ì •ë°€ë„-ì¬í˜„ìœ¨ ê³¡ì„ (PR Curve)ì„ í†µí•´ ëª¨ë“  ì„ê³„ê°’ì—ì„œì˜ ì§€í‘œ ê³„ì‚°\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, y_probs)\n",
    "    \n",
    "    # 8. F1-Scoreë¥¼ ìµœëŒ€ë¡œ ë§Œë“œëŠ” ìµœì ì˜ ì„ê³„ê°’(Threshold) ì°¾ê¸°\n",
    "    # f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10) # 0 ë‚˜ëˆ„ê¸° ë°©ì§€\n",
    "    best_idx = np.argmax(f1_scores) # F1ì´ ê°€ì¥ ë†’ì€ ì¸ë±ìŠ¤ ì¶”ì¶œ\n",
    "    best_th = thresholds[best_idx] # í•´ë‹¹ ì§€ì ì˜ ì„ê³„ê°’ ì €ì¥\n",
    "    \n",
    "    # 9. ìµœì  ì„ê³„ê°’ì„ ì ìš©í•œ ìµœì¢… ì§€í‘œ ì‚°ì¶œ ë° ê²°ê³¼ ì €ì¥\n",
    "    rf_comparison_results.append({\n",
    "        \"Dataset\": name,\n",
    "        \"F1-Score\": f1_scores[best_idx],\n",
    "        \"Recall\": recalls[best_idx],\n",
    "        \"Precision\": precisions[best_idx],\n",
    "        \"ROC-AUC\": roc_auc_score(y_test, y_probs),\n",
    "        \"Best_Threshold\": best_th\n",
    "    })\n",
    "\n",
    "# 10. ì „ì²´ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í•œëˆˆì— ë¹„êµ\n",
    "rf_results_df = pd.DataFrame(rf_comparison_results)\n",
    "print(\"\\n[ğŸ† ëœë¤í¬ë ˆìŠ¤íŠ¸ ë°ì´í„°ì…‹ë³„ ìµœì¢… ì„±ëŠ¥ ë¹„êµ]\")\n",
    "print(rf_results_df.sort_values(by=\"F1-Score\", ascending=False).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35d23cc-06f2-4201-864f-c89bbdcb4d28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
