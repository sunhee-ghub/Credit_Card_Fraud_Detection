import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
import numpy as np
import joblib
import gc
from sklearn.metrics import precision_recall_curve, f1_score, roc_auc_score, recall_score, precision_score

# --- [1. ì¥ì¹˜ ì„¤ì • ë° ê³µí†µ ë°ì´í„° ë¡œë“œ] ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
data_variants = ['org', 'smote', 'cgan', 'kcgan']

print(f"ğŸ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¡œë“œ ì¤‘... (Device: {device})")
X_test_scaled = joblib.load('X_test_scaled.pkl')
y_test = joblib.load('y_test.pkl')
X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)

# --- [2. TabTransformer ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ì˜] ---
# Transformer Encoderë¥¼ ì‚¬ìš©í•˜ì—¬ í”¼ì²˜ ê°„ì˜ ìƒí˜¸ì‘ìš©ì„ í•™ìŠµí•©ë‹ˆë‹¤.
class TabTransformer(nn.Module):
    def __init__(self, input_dim, embed_dim, num_heads, num_layers, dropout=0.1):
        super(TabTransformer, self).__init__()
        self.embed_dim = embed_dim
        # ìˆ˜ì¹˜í˜• í”¼ì²˜ë¥¼ ì„ë² ë”© ê³µê°„ìœ¼ë¡œ íˆ¬ì‚¬
        self.input_projection = nn.Linear(input_dim, input_dim * embed_dim)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim, nhead=num_heads, dim_feedforward=embed_dim * 4,
            dropout=dropout, batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        self.mlp = nn.Sequential(
            nn.Linear(input_dim * embed_dim, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1) # ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì¶œë ¥ì¸µ
        )

    def forward(self, x):
        # [Batch, Input_Dim] -> [Batch, Input_Dim, Embed_Dim]
        x = self.input_projection(x).view(x.size(0), -1, self.embed_dim)
        x = self.transformer_encoder(x)
        x = x.reshape(x.size(0), -1)
        return self.mlp(x)

# --- [3. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ë£¨í”„] ---
tab_results = []

for variant in data_variants:
    print(f"\n" + "="*50)
    print(f"ğŸš€ TabTransformer í•™ìŠµ ì‹œì‘: [{variant.upper()}] ë°ì´í„°ì…‹")
    print("="*50)
    
    # 1) ë°ì´í„°ì…‹ ë¡œë“œ
    X_tr = joblib.load(f'X_train_{variant}.pkl')
    y_tr = joblib.load(f'y_train_{variant}.pkl')
    
    input_dim = X_tr.shape[1]
    train_loader = DataLoader(
        TensorDataset(torch.FloatTensor(X_tr), torch.FloatTensor(y_tr).view(-1, 1)), 
        batch_size=1024, shuffle=True
    )
    
    # 2) ëª¨ë¸ ë° ìµœì í™” ì„¤ì •
    # pos_weight=4.0: ì‚¬ê¸° ë°ì´í„°(1)ì— 4ë°°ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë‘ì–´ ë¶ˆê· í˜• í•´ì†Œ
    model = TabTransformer(input_dim, embed_dim=32, num_heads=8, num_layers=3).to(device)
    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([4.0]).to(device))
    optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)

    # 3) í•™ìŠµ (Training)
    model.train()
    for epoch in range(30): # 30 ì—í¬í¬ í•™ìŠµ
        total_loss = 0
        for bx, by in train_loader:
            bx, by = bx.to(device), by.to(device)
            optimizer.zero_grad()
            outputs = model(bx)
            loss = criterion(outputs, by)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        scheduler.step()
        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/30] - Loss: {total_loss/len(train_loader):.4f}")

    # 4) í‰ê°€ (Evaluation)
    model.eval()
    with torch.no_grad():
        # ì˜ˆì¸¡ í™•ë¥  ê³„ì‚° (Sigmoid ì ìš©)
        logits = model(X_test_tensor)
        tab_probs = torch.sigmoid(logits).cpu().numpy().flatten()
    
    # Precision-Recall Curveë¥¼ í†µí•´ ìµœì ì˜ F1-Scoreì™€ ì„ê³„ê°’ ì°¾ê¸°
    precisions, recalls, thresholds = precision_recall_curve(y_test, tab_probs)
    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)
    best_idx = np.argmax(f1_scores)
    
    tab_results.append({
        "Dataset": variant.upper(),
        "F1-Score": f1_scores[best_idx],
        "Recall": recalls[best_idx],
        "Precision": precisions[best_idx],
        "ROC-AUC": roc_auc_score(y_test, tab_probs),
        "Best_Threshold": thresholds[best_idx] if best_idx < len(thresholds) else 0.5
    })

    # 5) ë©”ëª¨ë¦¬ ì •ë¦¬
    del X_tr, y_tr, model, train_loader
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

# --- [4. ìµœì¢… ê²°ê³¼ ì¶œë ¥] ---
print("\n" + "âœ¨" * 25)
print("ğŸ† TabTransformer ìµœì¢… ì‹¤í—˜ ê²°ê³¼ ë¦¬í¬íŠ¸")
print("âœ¨" * 25)
tab_df = pd.DataFrame(tab_results)
pd.options.display.float_format = '{:.4f}'.format
print(tab_df.sort_values(by="F1-Score", ascending=False).to_string(index=False))